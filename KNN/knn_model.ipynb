{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "X_train = []\n",
    "Y_train = []\n",
    "numerical_categories={}\n",
    "# open .tsv file\n",
    "with open(\"train_topics_keywords.tsv\", encoding='utf-8') as file:\n",
    "       \n",
    "    # Passing the TSV file to reader() function with tab delimiter. This function will read data from file\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    # printing data line by line\n",
    "    for line in tsv_file:\n",
    "        Y_train.append(line[1])\n",
    "        X_train.append(line[2])\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    numerical_categories[Y_train[i]] = i\n",
    "Y_train = list(numerical_categories.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XTrain</th>\n",
       "      <th>YTrain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cultivated,agricultural,maize,corn,fruit,wheat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reptile,lizard,salamander,fossil,frog,prehisto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>astronomer,astronomy,astrophysicist,mathematic...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aviation,airfield,airport,aerospace,aircraft,s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>actor,cast,screenwriter,filmmaker,film,actress...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>singer,musician,songwriter,guitarist,frontman,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>philosopher,scientist,mathematician,physicist,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bird,island,plumage,falconidae,pheasant,wildli...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>novel,trilogy,paperback,miniseries,folklore,bo...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>financier,businessman,roosevelt,richest,forbes...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pharmacology,sulfur,sulfuric,sulfurous,medicat...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>charlemagne,cathedral,pope,babylon,gothic,hymn...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hamburg,london,berlin,kyiv,riga,lviv,guangzhou...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>composer,musician,piano,libretto,concerto,orch...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>programming,java,computing,compiler,programmer...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>software,macromedia,multimedia,freeware,micros...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cricket,batsman,bowler,wicket,fielding,cricket...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>convicted,mp,homicide,fbi,murderer,criminal,in...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cyclist,cycling,giro,racing,competed,vuelta,ri...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hydroelectric,hydroelectricity,dam,river,flood...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>engineer,computer,hardware,engineering,worksta...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>screenplay,film,cartoon,cast,comic,musical,sou...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mahabharata,prussia,baltic,slavic,charlemagne,...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sea,valley,river,gulf,ocean,oceanic,coastline,...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>slavery,enslaved,slave,klan,constitution,mamlu...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>insect,larva,mosquito,larval,pest,wasp,parasit...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>muhammad,muslim,caliph,prophet,quran,caliphate...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>japan,japanese,tokyo,nikkei,meiji,tokugawa,nik...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>japanese,nippon,takashi,akihito,aoki,japan,nar...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>judah,jewish,judaism,jerusalem,testament,judea...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hindi,bengali,marathi,arabic,indian,syriac,kan...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>novelist,literature,writer,poetry,romanticism,...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>medication,pharmacology,herbal,herb,stimulant,...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>prussia,mamluk,mamluks,mameluk,mamlukization,p...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>regiment,regimental,jacobite,infantry,brigade,...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>admiral,naval,captain,fleet,navy,seaman,ensign...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>motorsport,racing,prix,motorsport,motorsports,...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ruud,eindhoven,arubans,aruba,aruban,arubano,ve...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>photographer,camera,photographic,photography,p...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>diplomat,constituency,politician,representativ...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ski,skiing,olympics,olympic,skier,medalist,med...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cambodia,thai,myanmar,thailand,burmese,lao,vie...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>cartoon,cast,itv,acting,actress,actor,spongebo...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tennis,wimbledon,federer,djokovic,nadal,novak,...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>trains,railway,rail,railroad,tramlink,crossrai...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>video games,arcade,gaming,nintendo,sega,playst...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               XTrain  YTrain\n",
       "0   cultivated,agricultural,maize,corn,fruit,wheat...       0\n",
       "1   reptile,lizard,salamander,fossil,frog,prehisto...       1\n",
       "2   astronomer,astronomy,astrophysicist,mathematic...       2\n",
       "3   aviation,airfield,airport,aerospace,aircraft,s...       3\n",
       "4   actor,cast,screenwriter,filmmaker,film,actress...       4\n",
       "5   singer,musician,songwriter,guitarist,frontman,...       5\n",
       "6   philosopher,scientist,mathematician,physicist,...       6\n",
       "7   bird,island,plumage,falconidae,pheasant,wildli...       7\n",
       "8   novel,trilogy,paperback,miniseries,folklore,bo...       8\n",
       "9   financier,businessman,roosevelt,richest,forbes...       9\n",
       "10  pharmacology,sulfur,sulfuric,sulfurous,medicat...      10\n",
       "11  charlemagne,cathedral,pope,babylon,gothic,hymn...      11\n",
       "12  hamburg,london,berlin,kyiv,riga,lviv,guangzhou...      12\n",
       "13  composer,musician,piano,libretto,concerto,orch...      13\n",
       "14  programming,java,computing,compiler,programmer...      14\n",
       "15  software,macromedia,multimedia,freeware,micros...      15\n",
       "16  cricket,batsman,bowler,wicket,fielding,cricket...      16\n",
       "17  convicted,mp,homicide,fbi,murderer,criminal,in...      17\n",
       "18  cyclist,cycling,giro,racing,competed,vuelta,ri...      18\n",
       "19  hydroelectric,hydroelectricity,dam,river,flood...      19\n",
       "20  engineer,computer,hardware,engineering,worksta...      20\n",
       "21  screenplay,film,cartoon,cast,comic,musical,sou...      21\n",
       "22  mahabharata,prussia,baltic,slavic,charlemagne,...      22\n",
       "23  sea,valley,river,gulf,ocean,oceanic,coastline,...      23\n",
       "24  slavery,enslaved,slave,klan,constitution,mamlu...      24\n",
       "25  insect,larva,mosquito,larval,pest,wasp,parasit...      25\n",
       "26  muhammad,muslim,caliph,prophet,quran,caliphate...      26\n",
       "27  japan,japanese,tokyo,nikkei,meiji,tokugawa,nik...      27\n",
       "28  japanese,nippon,takashi,akihito,aoki,japan,nar...      28\n",
       "29  judah,jewish,judaism,jerusalem,testament,judea...      29\n",
       "30  hindi,bengali,marathi,arabic,indian,syriac,kan...      30\n",
       "31  novelist,literature,writer,poetry,romanticism,...      31\n",
       "32  medication,pharmacology,herbal,herb,stimulant,...      32\n",
       "33  prussia,mamluk,mamluks,mameluk,mamlukization,p...      33\n",
       "34  regiment,regimental,jacobite,infantry,brigade,...      34\n",
       "35  admiral,naval,captain,fleet,navy,seaman,ensign...      35\n",
       "36  motorsport,racing,prix,motorsport,motorsports,...      36\n",
       "37  ruud,eindhoven,arubans,aruba,aruban,arubano,ve...      37\n",
       "38  photographer,camera,photographic,photography,p...      38\n",
       "39  diplomat,constituency,politician,representativ...      39\n",
       "40  ski,skiing,olympics,olympic,skier,medalist,med...      40\n",
       "41  cambodia,thai,myanmar,thailand,burmese,lao,vie...      41\n",
       "42  cartoon,cast,itv,acting,actress,actor,spongebo...      42\n",
       "43  tennis,wimbledon,federer,djokovic,nadal,novak,...      43\n",
       "44  trains,railway,rail,railroad,tramlink,crossrai...      44\n",
       "45  video games,arcade,gaming,nintendo,sega,playst...      45"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"XTrain\":X_train,\"YTrain\":Y_train}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'cheese', 'document', 'first', 'is', 'one', 'second', 'the',\n",
       "       'third', 'this', 'time'], dtype=object)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "X_Ai =  [\n",
    "     'This is the first document cheese.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first time?',\n",
    " ]\n",
    "Y_Ai = [1,2,3,4]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# features__=vectorizer.fit_transform(words)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bowVect = bow_vectorizer.fit(X_Ai)\n",
    "bowTrain = bow_vectorizer.transform(X_Ai)\n",
    "bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "# def cos_similarity(textlist):\n",
    "#     tfidf = TfidfVec.fit_transform(textlist)\n",
    "#     return tfidf\n",
    "# data = {'features':features__,\n",
    "#         'target': categories}\n",
    "# df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bowTrain.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_= []\n",
    "for i in df['features']:\n",
    "    features_.append(cos_similarity(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "clf = knn.fit(bowTrain, Y_Ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "res=clf.predict(bow_vectorizer.transform([\"second\"]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sssss'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*\"s\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Agriculture': 0, 'Amphibians and Reptiles': 1, 'Astronomy': 2, 'Aviation': 3, 'Biography/WikiProject Actors and Filmmakers': 4, 'Biography/WikiProject Musicians': 5, 'Biography/science and academia work group': 6, 'Birds': 7, 'Books': 8, 'Business': 9, 'Chemicals': 10, 'Christianity': 11, 'Cities': 12, 'Classical music': 13, 'Computer science': 14, 'Computing': 15, 'Cricket': 16, 'Crime and Criminal Biography': 17, 'Cycling': 18, 'Dams': 19, 'Engineering': 20, 'Film/American cinema task force': 21, 'Former countries': 22, 'Geography': 23, 'Human rights': 24, 'Insects': 25, 'Islam': 26, 'Japan': 27, 'Japan/Biography task force': 28, 'Jewish history': 29, 'Languages': 30, 'Literature': 31, 'Medicine': 32, 'Middle Ages': 33, 'Military history': 34, 'Military history/Maritime warfare task force': 35, 'Motorsport': 36, 'Netherlands': 37, 'Photography': 38, 'Politics': 39, 'Skiing and Snowboarding': 40, 'Southeast Asia': 41, 'Television': 42, 'Tennis': 43, 'Trains': 44, 'Video games': 45}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [323], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m  l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_train[x]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m  \u001b[39m#print(l,X_train[x])\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m  lst \u001b[39m=\u001b[39m l\u001b[39m*\u001b[39m(numerical_categories[x]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39m# print(len(lst.split(\",\")))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m  categories_repeats\u001b[39m.\u001b[39mappend(lst[:\u001b[39mlen\u001b[39m(lst)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "#X_TRAIN INTO ONE LONG LIST\n",
    "categories_repeats=[]\n",
    "print(numerical_categories)\n",
    "for x in range(len(X_train)):\n",
    "    l = len(X_train[x].split(\",\"))\n",
    "    #print(l,X_train[x])\n",
    "    lst = l*(numerical_categories[x]+\",\")\n",
    "   # print(len(lst.split(\",\")))\n",
    "    categories_repeats.append(lst[:len(lst)-1])\n",
    "\n",
    "Y_testing_train = \",\".join(categories_repeats).split(\",\")\n",
    "X_testing_train = \",\".join(X_train).split(\",\")\n",
    "data_test = {\"x\":X_testing_train, \"y\":[numerical_categories[y] for y in Y_testing_train]}\n",
    "df_test = pd.DataFrame(data_test)\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2898 2898\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_testing_train),len(X_testing_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aachen', 'abbasid', 'abbasids', ..., 'zoologist', 'zvonereva',\n",
       "       'ʿalī'], dtype=object)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# features__=vectorizer.fit_transform(words)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bowVect = bow_vectorizer.fit(X_testing_train)\n",
    "bowTrain = bow_vectorizer.transform(X_testing_train)\n",
    "bow_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business' 'Agriculture' 'Business' 'Cities' 'Cities' 'Cities'\n",
      " 'Agriculture' 'Human rights' 'Japan/Biography task force']\n"
     ]
    }
   ],
   "source": [
    "knn_test = KNeighborsClassifier(n_neighbors=1)\n",
    "# training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "clf_test = knn.fit(bowTrain, Y_testing_train)\n",
    "res=clf_test.predict(bow_vectorizer.transform([\"farm\", \"chicken\",\"land\",\",manchester\",\"milan\",\"westminster\",\"maize,corn,fruit,wheat,agriculture\",\",constitution,mamluk,\",\"takashi\"]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computing' 'Chemicals' 'Business' 'Tennis' 'Birds' 'Computer science']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train_temp = [] \n",
    "Y_train_temp = []\n",
    "\n",
    "categories_repeats=[]\n",
    "numerical_categories={}\n",
    "\n",
    "with open(\"train_topics_keywords.tsv\", encoding='utf-8') as file: #load tsv\n",
    "       \n",
    "    # Passing the TSV file to reader() function with tab delimiter. This function will read data from file\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    # printing data line by line\n",
    "    for line in tsv_file:\n",
    "        Y_train_temp.append(line[1]) # targets\n",
    "        X_train_temp.append(line[2]) # words related to a target\n",
    "\n",
    "\n",
    "for i in range(len(X_train_temp)):\n",
    "    numerical_categories[i] = Y_train_temp[i] # Assigning category a number\n",
    "Y_train_temp = list(numerical_categories.keys())\n",
    "\n",
    "for x in range(len(X_train_temp)):\n",
    "    l = len(X_train_temp[x].split(\",\"))\n",
    "    lst = l*(numerical_categories[x]+\",\")\n",
    "    categories_repeats.append(lst[:len(lst)-1])\n",
    "\n",
    "X_train = \",\".join(X_train_temp).split(\",\")\n",
    "Y_train = \",\".join(categories_repeats).split(\",\")\n",
    "\n",
    "data = {\"X_train\":X_train,\n",
    "\"Y_train\":Y_train}\n",
    "\n",
    "\n",
    "bow_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "bowVect = bow_vectorizer.fit(X_train)\n",
    "bowTrain = bow_vectorizer.transform(X_train)\n",
    "bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "clf = knn.fit(bowTrain, Y_train)\n",
    "res=clf.predict(bow_vectorizer.transform([\"window\",\"sulfuric\",\"hill\",\"wimbledon\", \"falconidae wii\",\" computer science\"]))\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "729903d8605f3711f162f2deb8ff557356f0f611eb1f3f8fa4a18667556a000a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
